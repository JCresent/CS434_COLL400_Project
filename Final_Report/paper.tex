% TODO fill in your paper title
\newcommand{\PaperTitle}{A Comparative Analysis of Machine Learning Algorithms for Website Traffic Classification from Network Packets }
% TODO fill in your paper number when you get it
\newcommand{\PaperNumber}{XXX}

\documentclass[10pt,sigconf,letterpaper,nonacm]{acmart}

\DeclareMathSizes{12}{30}{16}{12}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is the preamble; include packages as you see fit.
% Here are a few recommendations:
% \usepackage{color}
\usepackage{graphicx}
% \usepackage[labelformat=simple]{subcaption}
% \usepackage{xspace}
% \usepackage{multirow}
% \usepackage[ruled,vlined]{algorithm2e}
% \usepackage{ulem}
% \normalem

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{\PaperTitle}

\author{Matthew Berthoud, Lake Bradford, Justin Cresent, Will Katabian}
\affiliation{
  \institution{William \& Mary}
  \city{Williamsburg}
  \state{Virginia}
  \country{USA}
}

\begin{abstract}
Accurately Identifying web traffic destination and origins is crucial for the efficiency of a network. This project explores the potential of machine learning in reference to web traffic classification based on the 
analysis of network packets. 
We monitored and analyzed web traffic data from ChatGPT, Blackboard, and Linkedin, with the objective 
of building models which will be able to predict the web traffic origin of a specific packet. The collection of data was performed
using Wireshark, then the data was reformatted to eliminate bias and get more accurate results.
Using the collected data we then trained four models which had varying levels of accuracy, Logistic Regression (56\%), 
K-Nearest Neighbors (77\%), Random Forest (78\%), and finally a neural
network (80\%). This project shows the importance of machine learning within the field of 
network traffic analysis as automaton is much more efficient and precise compared to manually
examining web traffic, especially in a scale as large as the internet. One example of our project's significance is that this can be crucial data analysis 
for network administrators and security professionals, whom would examine the network for malicious traffic.

\end{abstract}

\keywords{Network Traffic, Machine Learning, Web Traffic Classification, Network Packets, Data Analysis, Neural Networks, Random Forest, K-Nearest Neighbors, Logistic Regression}

\maketitle

\section{Introduction}
The ability to monitor and analyze network traffic is crucial for the efficiency and security of a network \cite{10.1145/2388576.2388608}. 
It helps to manage the overall network performance, detect and prevent malicious activities, and ensure the network is operating as intended.
The present day internet is composed of a vast variety of diverse web traffic, of which requires a more sophisticated approach to analyze and classify network traffic\cite{10.5555/3432601.3432608}.
This project investigates a variety of machine learning algorithms to see which most accurately and effectively classify web traffic based on data from a set of captured network packets. 

  Traffic classification is very significant in practice, if done accurately and effectively \cite{10.1109/TNET.2014.2320577}. For reasons mentioned previously, the observed capabilities of this and other projects 
  can be crucial for network administrators and security professionals, whom would examine the network for malicious traffic.

  For this project, many sets of packet data were collected from three popular websites: ChatGPT, Blackboard, and Linkedin. These sets were then merged into a single dataset, which was then split into training and testing sets. 
  Four models were then trained and tested on this data: Logistic Regression, K-Nearest Neighbors, Random Forest, and Neural Network \cite{scikit-learn}. 

  A thorough evaluation of the models was conducted through testing and validation sets, hyperparameter optimization employing cross-validation and grid search, and computing accuracy and Macro F1 scores. The results showed that the Neural Network model 
  acheived the highest accuracy of $80\%$ and Macro F1 score of BLANK. %%TODO fill in the score.%%
  These scores display the model's ability to accurately classify the selected websites based on captured network packet data.

  Overall, the significance of this project is seen in its use of Machine Learning and the extensions of these applications into the discipline of network traffic analysis. This practice has potential to be utilized in real-world applications, and many sources
  proving it already is \cite{10.5555/3432601.3432608}. 

\section{Proposed Method}
This section presents the methodology for how this project goes about capturing network packet data, analyzing the data, and classifying it based on its website of origin.

\subsection{Capturing Data}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{Figures_and_Graphs/webCollectionDiagram.png}
  \caption{Wireshark Data Collection Process}
  \label{fig:dataCollection}
\end{figure*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{Figures_and_Graphs/wiresharkFigure.png}
  \includegraphics[width=0.5\textwidth]{Figures_and_Graphs/wiresharkDataFigure.png}
  \caption{Wireshark interface and example of captured data}
  \label{fig:wireshark}
\end{figure*}
\subsubsection{Network analysis tool} For this project, the tool chosen and used was Wireshark \cite{Wireshark}, a widely used network protocol analyzer. Wireshark is capable of capturing and analyzing network packets, and is able to provide a detailed
 view of the packets exchanged between the client and server. Wireshark offers a broad variety of features that allow for users to capture, decode, and analyze network packets. It shows the packet data in a human-readable format, and provides a detailed view of the packet's contents.
 This data contains information on time taken for packet to send and arrive, protocols used, size of each packet, and other fields such as more information on the packet and the source IP address. This tool also supports use of different forms of link communication and can be specified to capture packets from a specific network interface (ie. Ethernet, Wifi, etc.). 
 The many applications and ease of use of Wireshark make it a great tool for this project.
 
 The process for this project (shown in figure \ref{fig:dataCollection}) involved setting a hostname filter for each desired website, capturing packets from the host, and then exporting the data to a CSV file.

 \subsubsection{Data Collection}
To commence the collection of our data, we open Wireshark and select a network connection. 
In this project we established a connection the College of William and Mary's network interface (en0). In order to isolate the web traffic in order to specifically collect 
packets that we are analyzing, the capture filter functionality is utilized. The website of interest is then visited in the web browser which causes Wireshark to begin collecting the packets being exchanged.
After collecting sufficient packets, the data is then converted into a Comma Separated Values (CSV). This process of data collection is performed for Blackboard, LinkedIn, and ChatGPT resulting in the creation of 
three distinct files. 

\subsection{Data Preparation and Processing}
In order to ensure the integrity and reliability of our models,
the collected data must be processed prior to model training.


\subsubsection{Feature Selection}
The data collection process using Wireshark yields a dataset containing seven distinct features shown in figure \ref{fig:wireshark}. 
\begin{itemize}
  \item \textbf{Packet Number}: An integer representing the order in which the packets were captured during the Wireshark session. Provides insight on the order of the packets on the network interface. 
  \item \textbf{Time}: A floating point value representing the relative timestamp for each packet since the start of the Wireshark session. Similar to packet number, the time feature provides a better understanding of the packet order yet is more precise.
  \item \textbf{Source}: A string value containing the internet protocol address of the device that the packet originated from. 
  \item \textbf{Destination}: A string value containing the internet protocol address of the device that is receiving the packet. 
  \item \textbf{Protocol}: A string value which contains the network protocol employed for the transmission of the packet. Three distinct protocols were identified within the datasets used in this project (QUIC, TCP and TLSv1.2).
  \item \textbf{Length}: An integer representing the size of a packet in bits 
  \item \textbf{Info}: A string value containing more information pertaining to a packet. This value can vary heavily between packets.
\end{itemize} 
As the objective of this project is to predict the origin of web traffic, both source and destination are excluded from the dataset used to train the models. 
Similarly, the "info" feature is also removed from our train data as further testing revealed that the feature led to extraneous noise in our dataset. 
Consequently, the features selected for training and testing the models are Time, Length, and Protocol. The Protocol feature is converted to an integer for compatibility with 
the models using a dictionary to map protocol names to a corresponding numerical representation (e.g., 'TCP': 0, 'QUIC': 1, 'TLSv1.2': 2) as shown in figure \ref{fig:webTraffic}.  Additionally, a categorical variable titled "Website" 
is created which displays which website the packet originated (e.g., LinkedIn, ChatGPT, Blackboard). The "Website" feature acts as the target variable and allows 
the models to detect relationships between the packets and their corresponding web traffic origin. 

\subsubsection{Data Formatting}
To maintain normalized representation in all the collected datasets, a process of data harmonization 
was applied after collecting the necessary data. The dataset with the minimum number of observations is identified and accounted for when
truncating the remaining two datasets. Subsequently, the three datasets are combined and rearranged to minimize bias while training each of our models.
This process ensures a balanced yet unbiased dataset that maximizes the effectiveness of our models. Figure \ref{fig:webTraffic} presents the refined data following the data preparation. 
\begin{figure}[htp] % h: here, t: top, p: bottom
  \centering
  \includegraphics[width=0.5\textwidth]{Figures_and_Graphs/fullDataDiagram.png}
  \caption{Processed Web Traffic Data}
  \label{fig:webTraffic}
\end{figure}




\subsection{Employed models}
For this project, we required a classification model that would be able to accurately predict the website of origin for a given packet. We chose to employ three different models: K-Nearest Neighbors, Random Forest, and Neural Network. The reason we had selected more than one model was to be able to effectively compare the performance of each and determine which was the most effective for our solution. 
These models were chosen for their abilities to classify data and their potential to accurately predict the website of origin for a given packet. We also used a baseline model, Logistic Regression, to also compare the performance of the other models to a more simple, linear model.
\subsubsection{Logistic Regression (Baseline)}
As a starting point, we employed the Logistic Regression model as a baseline for our data analysis. Logistic regression is a statistical method used for binary classification tasks, where the goal is to predict the probability of an event happening or not happening.
It was understoof initiallly that this model may not be an ideal fit for our data; due to the regressions binary nature having more than two classifications is not ideal.

Characteristics of Logistic Regression:
\begin{enumerate}
  \item \textbf{Binary Outcome}: Logistic regression is used for binary classification tasks, where the outcome variable has only two possible outcome
  \item \textbf{Linear Relationship}: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the outcome
  \item \textbf{Probablistic Output}: Instead of predicting discrete classes directly, logistic regression predicts the probability that a given observation belongs to a particular class
  \item \textbf{Interpretability}: Logistic regression coefficients represent the change in the log-odds of the outcome for a one-unit change in the corresponding independent variable, making it easy to interpret the impact of each variable on the outcome. This characteristic was especially important for our baseline
\end{enumerate}

\begin{figure}[h!]
  \centering
  \includegraphics[width=9cm]{Figures_and_Graphs/LogReg.png}
  \caption{Logistic Regression model structure example}
  \label{fig:RFExample}
\end{figure}

\subsubsection{K-Nearest Neighbors}

\subsubsection{Random Forest}
A Random Forest is a form of an ensemble model that is composed of multiple decision trees. Each tree is ran and gives its predicted classification, and the final classification is determined by a majority vote of all the trees (seen in figure \ref{fig:RFExample}).

Advantages of Random Forest Models:
\begin{enumerate}
  \item \textbf{Ability to handle complex features}: Random forests have the ability to handle complex features, such as time and protocols in the case of network packets, and the relationships between them, which can be very important in classifying network traffic.
  \item \textbf{High accuracy}: Random forests diminishes the problem of overfitting that can occur in decision trees, and can achieve higher accuracy, helping to more accurately determine website origins of a packet.
  \item \textbf{Feature importance}: Random forests can provide insight into which features are most important in the classification process, such as protocol vs time in classifying websites for this project.
  \item \textbf{Scalability}: Random forests can be scaled to handle much larger datasets, which can be very important for network traffic which can receive a high volume of packet data extremely quick.
\end{enumerate}

The library used for the Random Forest model was scikit-learn \cite{scikit-learn}. Sci-kit learn's implementation of Random Forests utilize aggregation to take a soft vote of the soft classifications (probability of the packet being from each class). Within soft voting, a weighted average of the probabilities from the classification are taken, and the class with the highest probability is chosen as the final classification.

\begin{figure}[h!]
  \centering
  \includegraphics[width=9cm]{Figures_and_Graphs/RandomForestExample.png}
  \caption{Random Forest model structure example}
  \label{fig:RFExample}
\end{figure}

\subsubsection{Neural Network}
\section{Evaluation}
This section comprehensively evaluates our models and their performance on the dataset and validating sets. Later, goes on to determine the model which best classified our selected websites based on network packet data.
\subsection{Dataset}
\subsection{Evaluation Metrics}
Accuracy and Macro F1 score were the primary metrics used to evaluate each model. These metrics are applicable to all models and allow for
a direct comparison for each of their performances. Two other metrics: precision and recall, help to provide a more detailed understanding of the model's performance on a class-to-class basis.
\newline \\
The equation for \textbf{Accuracy}: \\
\[
Accuracy= \frac{TP + TN}{TS}
\]
\newline \\
The equation for \textbf{Macro F1}: \\
\[
Macro F 1= \frac{1}{n} \sum_{1}^{n} \frac{2*Precision_i*Recall_i}{Precision_i+Recall_i}
\]
\newline \\
The equation for \textbf{Recall}: \\
\[
Recall= \frac{TP}{TP + FN}
\]
\newline \\
The equation for \textbf{Precision}: \\
\[
Accuracy= \frac{TP}{TP + FP}
\]
\\
Accuracy is the score that rates the effectiveness of a model at classification overall (ie. how many it predicted correctly out of the total set). 

Macro F1 is the harmonic average of the of precision and recall for every class, and then taking the average of these calculated scores across all classes.

Recall is the calculation of how many classifications the model got correct (ie. how many truly belong in that class in the entire dataset). 
Precision is the calculation of how many classifications truly belong to each class (ie. can be thought of as class accuracy).


\subsection{Model Comparison and Evaluation Results}
The results in table \ref{tab:results} show that the neural network achieved the highest accuracy (80\%) and Macro F1 score (0.00). These results mean the model was the most effective at classifying the selected websites based on network packet data.
This displays the neural network is much more effective at representing complex relationships in the data than the other models. 

  \begin{table}[!h]
    \begin{center}
    \caption{Accuracy and Macro F1 Scores for Each Model}
    \label{tab:results}
    \begin{tabular}{|c|c|c|}
      \hline
      Model & Accuracy & Macro F1 \\
      \hline
      Logistic Regression & 56\% & 0.00 \\
      K-Nearest Neighbors & 77\% & 0.00 \\
      Random Forest & 78\% & 0.00 \\
      Neural Network & 80\% & 0.00 \\
      \hline
    \end{tabular}
    \end{center}
  \end{table}

\section{Discussion \& Future Work}
\subsection{Analysis of Results}
\subsection{Future Work}

\section{Conclusion}

% Note from the CFP that this section must include a statement about
% ethical issues; papers that do not include such a statement may be
% rejected.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We're in the endgame now

\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}
\cite{10.1145/2388576.2388608}
\cite{10.5555/3432601.3432608}
\cite{10.1109/TNET.2014.2320577}
\cite{scikit-learn}
\cite{Wireshark}

\end{document}
